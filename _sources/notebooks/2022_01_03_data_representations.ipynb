{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d26f13f",
   "metadata": {},
   "source": [
    "## Data Representations\n",
    "\n",
    "**Object Representations**\n",
    "- In the real world, we have objects, which can be a document, image, DNA, etc. But for the computer to understand, we need a mathematical representations of these objects.\n",
    "- We have seen many mathematical representations before, such as:\n",
    "    - Documents → Sets/Vectors\n",
    "    - Images → Vector\n",
    "    - DNA → Sequence\n",
    "\n",
    "**Interaction Reperesentations**\n",
    "\n",
    "There are instances, where we need to represent interactions between objects, than the object itself.\n",
    "- We can use networks when showing the interactions between objects.\n",
    "- The network can be rich with edge weight, edge tags, etc.\n",
    "\n",
    "### Visualization of Images as Tensors\n",
    "- Tensors are a specialized data structure that are very similar to arrays and matrices.\n",
    "- The below code snippet visualizes the grayscale images from MNIST dataset in form of tensors with values between 0 to 255. 0 is for black and 255 for white, while the numbers in between are the different shades of gray.\n",
    "- The images are in grayscale here, but for normal images, there will be more than one channels. For example, the colored RGB images have 3 channels denoting intensity of red, green and blue vaues. This gives the shape of tensor as $(n\\_channels \\times height \\times width )$.\n",
    "\n",
    "\n",
    ":::{admonition} Click to show code\n",
    ":class: dropdown\n",
    "\n",
    "```python\n",
    "import torchvision\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from matplotlib import rc\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib import animation\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Settings for the animation\n",
    "rc('animation', html='jshtml')\n",
    "frn = 10 # Number of frames to process in the animation\n",
    "fps = 0.5 # Frames per second\n",
    "mywriter = animation.PillowWriter(fps=fps)\n",
    "\n",
    "# MNIST dataset\n",
    "mnist_dataset = torchvision.datasets.MNIST(root = \"data/mnist\", train = True, download = True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "if not os.path.exists(\"assets/gif/image0\"):\n",
    "    os.makedirs(\"assets/gif/image0\")\n",
    "\n",
    "# Plot gray channel tensor and print the pixel values\n",
    "for loop_idx, (image_tensor, label) in enumerate(mnist_dataset):\n",
    "    fig, ax = plt.subplots(figsize = (10, 10))\n",
    "    image_tensor_gray = image_tensor[0]\n",
    "    image_tensor_gray = image_tensor_gray * 255\n",
    "    ax.matshow(image_tensor_gray, cmap = \"gray\")\n",
    "    for i in range(image_tensor_gray.shape[0]):\n",
    "        for j in range(image_tensor_gray.shape[1]):\n",
    "            ax.text(i, j, str(int(image_tensor_gray[j][i].item())), va = \"center\", ha = \"center\", color = \"blue\", fontsize = \"small\")\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"assets/gif/image0/{loop_idx}.png\")\n",
    "    plt.close(fig)\n",
    "    if loop_idx >= frn:\n",
    "        break\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 10))\n",
    "\n",
    "plot = [ax.imshow(mpimg.imread(f\"assets/gif/image0/0.png\"))]\n",
    "def change_plot(frame_number):\n",
    "    plot[0].remove()\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plot[0] = ax.imshow(mpimg.imread(f\"assets/gif/image0/{frame_number}.png\"))\n",
    "\n",
    "ani = FuncAnimation(fig, change_plot, frn, interval=1000 / fps)\n",
    "plt.tight_layout()\n",
    "display(ani)\n",
    "ani.save('mnist_gray_values.gif',writer=mywriter)\n",
    "plt.clf()\n",
    "plt.close(fig)\n",
    "```\n",
    ":::\n",
    "\n",
    ":::{admonition} Tensor values in MNIST dataset\n",
    ":class: dropdown\n",
    "```{figure} ../assets/2022_01_03_data_representations/mnist_gray_values.gif\n",
    "---\n",
    "name: mnist-tensor\n",
    "---\n",
    "Tensor values in MNIST dataset images.\n",
    "(0 is for black, 255 for white and other values show different shades of gray)\n",
    "```\n",
    ":::\n",
    "\n",
    ":::{admonition} Tensor values in CIFAR dataset\n",
    ":class: dropdown\n",
    "```{figure} ../assets/2022_01_03_data_representations/cifar_frog_rgb.png\n",
    "---\n",
    "name: cifar-frog-tensor\n",
    "---\n",
    "Tensor values of an image of frog in CIFAR dataset.\n",
    "The first three images shows the values in Red, Green and Blue channels. The fourth image (original image) is the combined image, as we perceive.\n",
    "\n",
    "```{attention}\n",
    "Zoom-in to the image to view the values clearly.\n",
    "```\n",
    ":::\n",
    "\n",
    "### Document as \"Bag of Words\" model\n",
    "Now, we move to object representation of a document, which can be seen as a sequence of letters. But the distribution of frequency of letters in every English documents would be somewhat similar and not good for representation.\n",
    "Hence, we take individual words as the smallest unit.\n",
    "\n",
    "For Bag of Words model, there are multiple steps. We will explore the different steps with the help of an example from IMDB reviews dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf99ef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import re\n",
    "\n",
    "imdb_dataset = torchtext.datasets.IMDB(root = \"./data/imdb\", split = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024cd5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = next(imdb_dataset)\n",
    "label = item[0]\n",
    "review_text = item[1]\n",
    "display(review_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8779dd9",
   "metadata": {},
   "source": [
    "The chosen review paragraph is:\n",
    "<pre width=100%>\n",
    "I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn't match the background, and painfully one-dimensional characters cannot be overcome with a 'sci-fi' setting. (I'm sure there are those of you out there who think Babylon 5 is good sci-fi TV. It's not. It's clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It's really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it's rubbish as they have to always say \"Gene Roddenberry's Earth...\" otherwise people would not continue watching. Roddenberry's ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.\n",
    "</pre>\n",
    "\n",
    "````{panels}\n",
    ":column: col-12\n",
    ":card: border-4\n",
    "**Step 1: Breaking the text into sentences**\n",
    "^^^\n",
    "```python\n",
    "review_sent_tokens = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', review_text) # Use this regex instead of simple .split(\".\"), as review contains \"...\"\n",
    "display(review_sent_tokens)\n",
    "```\n",
    "\n",
    "```{note}\n",
    "I used Regex here instead of simple `split(.)`, as the text might contain `...` in the reviews.\n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "['I love sci-fi and am willing to put up with a lot.',\n",
    " 'Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood.',\n",
    " 'I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original).',\n",
    " \"Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn't match the background, and painfully one-dimensional characters cannot be overcome with a 'sci-fi' setting.\",\n",
    " \"(I'm sure there are those of you out there who think Babylon 5 is good sci-fi TV.\",\n",
    " \"It's not.\",\n",
    " \"It's clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf.\",\n",
    " 'Star Trek).',\n",
    " 'It may treat important issues, yet not as a serious philosophy.',\n",
    " \"It's really difficult to care about the characters here as they are not simply foolish, just missing a spark of life.\",\n",
    " 'Their actions and reactions are wooden and predictable, often painful to watch.',\n",
    " 'The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching.',\n",
    " \"Roddenberry's ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space.\",\n",
    " 'Spoiler.',\n",
    " 'So, kill off a main character.',\n",
    " 'And then bring him back as another actor.',\n",
    " 'Jeeez! Dallas all over again.']\n",
    "```\n",
    "---\n",
    "\n",
    "**Step 2: Breaking the sentences into words (tokenization)**\n",
    "^^^\n",
    "For simplicity, we demonstrate only on first five sentences in the chosen review text.\n",
    "```python\n",
    "review_sent_tokens = review_sent_tokens[:5] # Take only first 3 sentences for illustration\n",
    "def tokenise(sentence):\n",
    "    # split the sentence into units (words or phrases)\n",
    "    return re.findall(\"[A-Z]{2,}(?![a-z])|[A-Z][a-z]+(?=[A-Z])|[\\'\\w\\-]+\", sentence)\n",
    "\n",
    "review_word_tokens = [tokenise(sent) for sent in review_sent_tokens]\n",
    "print(*review_word_tokens, sep = \"\\n\")\n",
    "```\n",
    "\n",
    "```{note}\n",
    "Again use of complex Regex is better than simple `split()`.\n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "['I', 'love', 'sci-fi', 'and', 'am', 'willing', 'to', 'put', 'up', 'with', 'a', 'lot']\n",
    "['Sci-fi', 'movies', 'TV', 'are', 'usually', 'underfunded', 'under-appreciated', 'and', 'misunderstood']\n",
    "['I', 'tried', 'to', 'like', 'this', 'I', 'really', 'did', 'but', 'it', 'is', 'to', 'good', 'TV', 'sci-fi', 'as', 'Babylon', '5', 'is', 'to', 'Star', 'Trek', 'the', 'original']\n",
    "['Silly', 'prosthetics', 'cheap', 'cardboard', 'sets', 'stilted', 'dialogues', 'CG', 'that', \"doesn't\", 'match', 'the', 'background', 'and', 'painfully', 'one-dimensional', 'characters', 'cannot', 'be', 'overcome', 'with', 'a', \"'sci-fi'\", 'setting']\n",
    "[\"I'm\", 'sure', 'there', 'are', 'those', 'of', 'you', 'out', 'there', 'who', 'think', 'Babylon', '5', 'is', 'good', 'sci-fi', 'TV']\n",
    "```\n",
    "---\n",
    "\n",
    "**Step 3a: Stemming**\n",
    "^^^\n",
    "In stemming we reduce each word to its stem/root word by removing its prefix and/or suffix.\n",
    "For example: argue, argued, argues and arguing; all reduce to the stem word `argu`.\n",
    "\n",
    "Here we have used the Port-Stemmer algorithm available in nltk library\n",
    "```python3\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stem_sentence(sent_word_tokens):\n",
    "    return [ps.stem(word) for word in sent_word_tokens]\n",
    "\n",
    "review_word_stem_tokens = [stem_sentence(sent_word_tokens) for sent_word_tokens in review_word_tokens]\n",
    "print(\"Stemmer::\")\n",
    "print(*review_word_stem_tokens, sep = \"\\n\")\n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "Stemmer::\n",
    "['I', 'love', 'sci-fi', 'and', 'am', 'will', 'to', 'put', 'up', 'with', 'a', 'lot']\n",
    "['sci-fi', 'movi', 'TV', 'are', 'usual', 'underfund', 'under-appreci', 'and', 'misunderstood']\n",
    "['I', 'tri', 'to', 'like', 'thi', 'I', 'realli', 'did', 'but', 'it', 'is', 'to', 'good', 'TV', 'sci-fi', 'as', 'babylon', '5', 'is', 'to', 'star', 'trek', 'the', 'origin']\n",
    "['silli', 'prosthet', 'cheap', 'cardboard', 'set', 'stilt', 'dialogu', 'CG', 'that', \"doesn't\", 'match', 'the', 'background', 'and', 'pain', 'one-dimension', 'charact', 'cannot', 'be', 'overcom', 'with', 'a', \"'sci-fi'\", 'set']\n",
    "[\"i'm\", 'sure', 'there', 'are', 'those', 'of', 'you', 'out', 'there', 'who', 'think', 'babylon', '5', 'is', 'good', 'sci-fi', 'TV']\n",
    "```\n",
    "---\n",
    "\n",
    "**Step 3b: Lemmatization**\n",
    "^^^\n",
    "In Stemming, we were limited to removing prefix/suffix to convert word to its root word. But in lemmatization, we also convert group of similar meaning words to a single word, treating them as a single entity.\n",
    "\n",
    "For example: good and better are both converted to good in lemmatization.\n",
    "\n",
    "In code, we use the `WordNetLemmatizer` available in nltk library.\n",
    "```python\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_sentence(sent_word_tokens):\n",
    "    return [wordnet_lemmatizer.lemmatize(word) for word in sent_word_tokens]\n",
    "\n",
    "review_word_lemma_tokens = [lemmatize_sentence(sent_word_tokens) for sent_word_tokens in review_word_tokens]\n",
    "print(\"Lemmatizer::\")\n",
    "print(*review_word_lemma_tokens, sep = \"\\n\")\n",
    "```\n",
    "Output:\n",
    "```\n",
    "Lemmatizer::\n",
    "['I', 'love', 'sci-fi', 'and', 'am', 'willing', 'to', 'put', 'up', 'with', 'a', 'lot']\n",
    "['Sci-fi', 'movie', 'TV', 'are', 'usually', 'underfunded', 'under-appreciated', 'and', 'misunderstood']\n",
    "['I', 'tried', 'to', 'like', 'this', 'I', 'really', 'did', 'but', 'it', 'is', 'to', 'good', 'TV', 'sci-fi', 'a', 'Babylon', '5', 'is', 'to', 'Star', 'Trek', 'the', 'original']\n",
    "['Silly', 'prosthetics', 'cheap', 'cardboard', 'set', 'stilted', 'dialogue', 'CG', 'that', \"doesn't\", 'match', 'the', 'background', 'and', 'painfully', 'one-dimensional', 'character', 'cannot', 'be', 'overcome', 'with', 'a', \"'sci-fi'\", 'setting']\n",
    "[\"I'm\", 'sure', 'there', 'are', 'those', 'of', 'you', 'out', 'there', 'who', 'think', 'Babylon', '5', 'is', 'good', 'sci-fi', 'TV']\n",
    "```\n",
    "---\n",
    "\n",
    "**Step 4: Removing Stopwords**\n",
    "^^^\n",
    "Stopwords are the most common words such as \"the\", \"a\", \"an\", \"on\", etc. (for english) in a language. We can remove these words from processing to save time and space in processing, so that the other important words are focused upon.\n",
    "\n",
    "Here, in code, we simply check for each word if it is present in stopwords list, if yes, then we discard the word for further processing.\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stop_words(sent_word_tokens):\n",
    "    return [word for word in sent_word_tokens if word not in stopwords.words('english')]\n",
    "\n",
    "review_word_stem_tokens = [remove_stop_words(sent_word_tokens) for sent_word_tokens in review_word_stem_tokens]\n",
    "print(*review_word_stem_tokens, sep = \"\\n\")\n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "['I', 'love', 'sci-fi', 'put', 'lot']\n",
    "['sci-fi', 'movi', 'TV', 'usual', 'underfund', 'under-appreci', 'misunderstood']\n",
    "['I', 'tri', 'like', 'thi', 'I', 'realli', 'good', 'TV', 'sci-fi', 'babylon', '5', 'star', 'trek', 'origin']\n",
    "['silli', 'prosthet', 'cheap', 'cardboard', 'set', 'stilt', 'dialogu', 'CG', 'match', 'background', 'pain', 'one-dimension', 'charact', 'cannot', 'overcom', \"'sci-fi'\", 'set']\n",
    "[\"i'm\", 'sure', 'think', 'babylon', '5', 'good', 'sci-fi', 'TV']\n",
    "```\n",
    "````\n",
    "\n",
    "**Step 5: Building Unigrams, Bigrams, Trigrams, Skip-grams etc.**\n",
    "\n",
    "````{tabbed} Unigrams\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "unigram_vocab = {}\n",
    "for sent in review_word_stem_tokens:\n",
    "    for word in sent:\n",
    "        if word not in unigram_vocab:\n",
    "            unigram_vocab[word] = len(unigram_vocab)\n",
    "\n",
    "review_sent_count_vectors = np.zeros((len(review_word_stem_tokens), len(unigram_vocab)), dtype = np.int32)\n",
    "\n",
    "for sent_idx in range(len(review_word_stem_tokens)):\n",
    "    for word in review_word_stem_tokens[sent_idx]:\n",
    "        review_sent_count_vectors[sent_idx][unigram_vocab[word]] += 1\n",
    "\n",
    "df = pd.DataFrame(data = review_sent_count_vectors, columns = sorted(unigram_vocab, key=unigram_vocab.get))\n",
    "display(df)\n",
    "```\n",
    "```python\n",
    "print(df.to_markdown())\n",
    "```\n",
    "\n",
    "|    |   I |   love |   sci-fi |   put |   lot |   movi |   TV |   usual |   underfund |   under-appreci |   misunderstood |   tri |   like |   thi |   realli |   good |   babylon |   5 |   star |   trek |   origin |   silli |   prosthet |   cheap |   cardboard |   set |   stilt |   dialogu |   CG |   match |   background |   pain |   one-dimension |   charact |   cannot |   overcom |   'sci-fi' |   i'm |   sure |   think |\n",
    "|---:|----:|-------:|---------:|------:|------:|-------:|-----:|--------:|------------:|----------------:|----------------:|------:|-------:|------:|---------:|-------:|----------:|----:|-------:|-------:|---------:|--------:|-----------:|--------:|------------:|------:|--------:|----------:|-----:|--------:|-------------:|-------:|----------------:|----------:|---------:|----------:|-----------:|------:|-------:|--------:|\n",
    "|  0 |   1 |      1 |        1 |     1 |     1 |      0 |    0 |       0 |           0 |               0 |               0 |     0 |      0 |     0 |        0 |      0 |         0 |   0 |      0 |      0 |        0 |       0 |          0 |       0 |           0 |     0 |       0 |         0 |    0 |       0 |            0 |      0 |               0 |         0 |        0 |         0 |          0 |     0 |      0 |       0 |\n",
    "|  1 |   0 |      0 |        1 |     0 |     0 |      1 |    1 |       1 |           1 |               1 |               1 |     0 |      0 |     0 |        0 |      0 |         0 |   0 |      0 |      0 |        0 |       0 |          0 |       0 |           0 |     0 |       0 |         0 |    0 |       0 |            0 |      0 |               0 |         0 |        0 |         0 |          0 |     0 |      0 |       0 |\n",
    "|  2 |   2 |      0 |        1 |     0 |     0 |      0 |    1 |       0 |           0 |               0 |               0 |     1 |      1 |     1 |        1 |      1 |         1 |   1 |      1 |      1 |        1 |       0 |          0 |       0 |           0 |     0 |       0 |         0 |    0 |       0 |            0 |      0 |               0 |         0 |        0 |         0 |          0 |     0 |      0 |       0 |\n",
    "|  3 |   0 |      0 |        0 |     0 |     0 |      0 |    0 |       0 |           0 |               0 |               0 |     0 |      0 |     0 |        0 |      0 |         0 |   0 |      0 |      0 |        0 |       1 |          1 |       1 |           1 |     2 |       1 |         1 |    1 |       1 |            1 |      1 |               1 |         1 |        1 |         1 |          1 |     0 |      0 |       0 |\n",
    "|  4 |   0 |      0 |        1 |     0 |     0 |      0 |    1 |       0 |           0 |               0 |               0 |     0 |      0 |     0 |        0 |      1 |         1 |   1 |      0 |      0 |        0 |       0 |          0 |       0 |           0 |     0 |       0 |         0 |    0 |       0 |            0 |      0 |               0 |         0 |        0 |         0 |          0 |     1 |      1 |       1 |\n",
    "\n",
    "````\n",
    "\n",
    "````{tabbed} Bigrams\n",
    "```python\n",
    "bigram_vocab = {}\n",
    "for sent in review_word_stem_tokens:\n",
    "    for word_idx in range(len(sent) - 1):\n",
    "        bigram = sent[word_idx] + str(\" \") + sent[word_idx + 1]\n",
    "        if bigram not in bigram_vocab:\n",
    "            bigram_vocab[bigram] = len(bigram_vocab)\n",
    "\n",
    "review_sent_bigram_count_vectors = np.zeros((len(review_word_stem_tokens), len(bigram_vocab)), dtype = np.int32)\n",
    "\n",
    "for sent_idx in range(len(review_word_stem_tokens)):\n",
    "    for word_idx in range(len(review_word_stem_tokens[sent_idx]) - 1):\n",
    "        bigram = review_word_stem_tokens[sent_idx][word_idx] + str(\" \") + review_word_stem_tokens[sent_idx][word_idx + 1]\n",
    "        review_sent_bigram_count_vectors[sent_idx][bigram_vocab[bigram]] += 1\n",
    "\n",
    "df = pd.DataFrame(data = review_sent_bigram_count_vectors, columns = sorted(bigram_vocab, key=bigram_vocab.get))\n",
    "display(df)\n",
    "```\n",
    "```python\n",
    "print(df.to_markdown())\n",
    "```\n",
    "\n",
    "|    |   I love |   love sci-fi |   sci-fi put |   put lot |   sci-fi movi |   movi TV |   TV usual |   usual underfund |   underfund under-appreci |   under-appreci misunderstood |   I tri |   tri like |   like thi |   thi I |   I realli |   realli good |   good TV |   TV sci-fi |   sci-fi babylon |   babylon 5 |   5 star |   star trek |   trek origin |   silli prosthet |   prosthet cheap |   cheap cardboard |   cardboard set |   set stilt |   stilt dialogu |   dialogu CG |   CG match |   match background |   background pain |   pain one-dimension |   one-dimension charact |   charact cannot |   cannot overcom |   overcom 'sci-fi' |   'sci-fi' set |   i'm sure |   sure think |   think babylon |   5 good |   good sci-fi |   sci-fi TV |\n",
    "|---:|---------:|--------------:|-------------:|----------:|--------------:|----------:|-----------:|------------------:|--------------------------:|------------------------------:|--------:|-----------:|-----------:|--------:|-----------:|--------------:|----------:|------------:|-----------------:|------------:|---------:|------------:|--------------:|-----------------:|-----------------:|------------------:|----------------:|------------:|----------------:|-------------:|-----------:|-------------------:|------------------:|---------------------:|------------------------:|-----------------:|-----------------:|-------------------:|---------------:|-----------:|-------------:|----------------:|---------:|--------------:|------------:|\n",
    "|  0 |        1 |             1 |            1 |         1 |             0 |         0 |          0 |                 0 |                         0 |                             0 |       0 |          0 |          0 |       0 |          0 |             0 |         0 |           0 |                0 |           0 |        0 |           0 |             0 |                0 |                0 |                 0 |               0 |           0 |               0 |            0 |          0 |                  0 |                 0 |                    0 |                       0 |                0 |                0 |                  0 |              0 |          0 |            0 |               0 |        0 |             0 |           0 |\n",
    "|  1 |        0 |             0 |            0 |         0 |             1 |         1 |          1 |                 1 |                         1 |                             1 |       0 |          0 |          0 |       0 |          0 |             0 |         0 |           0 |                0 |           0 |        0 |           0 |             0 |                0 |                0 |                 0 |               0 |           0 |               0 |            0 |          0 |                  0 |                 0 |                    0 |                       0 |                0 |                0 |                  0 |              0 |          0 |            0 |               0 |        0 |             0 |           0 |\n",
    "|  2 |        0 |             0 |            0 |         0 |             0 |         0 |          0 |                 0 |                         0 |                             0 |       1 |          1 |          1 |       1 |          1 |             1 |         1 |           1 |                1 |           1 |        1 |           1 |             1 |                0 |                0 |                 0 |               0 |           0 |               0 |            0 |          0 |                  0 |                 0 |                    0 |                       0 |                0 |                0 |                  0 |              0 |          0 |            0 |               0 |        0 |             0 |           0 |\n",
    "|  3 |        0 |             0 |            0 |         0 |             0 |         0 |          0 |                 0 |                         0 |                             0 |       0 |          0 |          0 |       0 |          0 |             0 |         0 |           0 |                0 |           0 |        0 |           0 |             0 |                1 |                1 |                 1 |               1 |           1 |               1 |            1 |          1 |                  1 |                 1 |                    1 |                       1 |                1 |                1 |                  1 |              1 |          0 |            0 |               0 |        0 |             0 |           0 |\n",
    "|  4 |        0 |             0 |            0 |         0 |             0 |         0 |          0 |                 0 |                         0 |                             0 |       0 |          0 |          0 |       0 |          0 |             0 |         0 |           0 |                0 |           1 |        0 |           0 |             0 |                0 |                0 |                 0 |               0 |           0 |               0 |            0 |          0 |                  0 |                 0 |                    0 |                       0 |                0 |                0 |                  0 |              0 |          1 |            1 |               1 |        1 |             1 |           1 |\n",
    "\n",
    "````\n",
    "\n",
    "````{tabbed} Trigrams\n",
    "```python3\n",
    "trigram_vocab = {}\n",
    "for sent in review_word_stem_tokens:\n",
    "    for word_idx in range(len(sent) - 2):\n",
    "        trigram = sent[word_idx] + str(\" \") + sent[word_idx + 1] + str(\" \") + sent[word_idx + 2]\n",
    "        if trigram not in trigram_vocab:\n",
    "            trigram_vocab[trigram] = len(trigram_vocab)\n",
    "\n",
    "review_sent_trigram_count_vectors = np.zeros((len(review_word_stem_tokens), len(trigram_vocab)), dtype = np.int32)\n",
    "\n",
    "for sent_idx in range(len(review_word_stem_tokens)):\n",
    "    for word_idx in range(len(review_word_stem_tokens[sent_idx]) - 2):\n",
    "        trigram = review_word_stem_tokens[sent_idx][word_idx] + str(\" \") + review_word_stem_tokens[sent_idx][word_idx + 1] + str(\" \") + review_word_stem_tokens[sent_idx][word_idx + 2]\n",
    "        review_sent_trigram_count_vectors[sent_idx][trigram_vocab[trigram]] += 1\n",
    "\n",
    "df = pd.DataFrame(data = review_sent_trigram_count_vectors, columns = sorted(trigram_vocab, key=trigram_vocab.get))\n",
    "display(df)\n",
    "```\n",
    "\n",
    "```python3\n",
    "print(df.to_markdown())\n",
    "```\n",
    "\n",
    "\n",
    "|    |   I love sci-fi |   love sci-fi put |   sci-fi put lot |   sci-fi movi TV |   movi TV usual |   TV usual underfund |   usual underfund under-appreci |   underfund under-appreci misunderstood |   I tri like |   tri like thi |   like thi I |   thi I realli |   I realli good |   realli good TV |   good TV sci-fi |   TV sci-fi babylon |   sci-fi babylon 5 |   babylon 5 star |   5 star trek |   star trek origin |   silli prosthet cheap |   prosthet cheap cardboard |   cheap cardboard set |   cardboard set stilt |   set stilt dialogu |   stilt dialogu CG |   dialogu CG match |   CG match background |   match background pain |   background pain one-dimension |   pain one-dimension charact |   one-dimension charact cannot |   charact cannot overcom |   cannot overcom 'sci-fi' |   overcom 'sci-fi' set |   i'm sure think |   sure think babylon |   think babylon 5 |   babylon 5 good |   5 good sci-fi |   good sci-fi TV |\n",
    "|---:|----------------:|------------------:|-----------------:|-----------------:|----------------:|---------------------:|--------------------------------:|----------------------------------------:|-------------:|---------------:|-------------:|---------------:|----------------:|-----------------:|-----------------:|--------------------:|-------------------:|-----------------:|--------------:|-------------------:|-----------------------:|---------------------------:|----------------------:|----------------------:|--------------------:|-------------------:|-------------------:|----------------------:|------------------------:|--------------------------------:|-----------------------------:|-------------------------------:|-------------------------:|--------------------------:|-----------------------:|-----------------:|---------------------:|------------------:|-----------------:|----------------:|-----------------:|\n",
    "|  0 |               1 |                 1 |                1 |                0 |               0 |                    0 |                               0 |                                       0 |            0 |              0 |            0 |              0 |               0 |                0 |                0 |                   0 |                  0 |                0 |             0 |                  0 |                      0 |                          0 |                     0 |                     0 |                   0 |                  0 |                  0 |                     0 |                       0 |                               0 |                            0 |                              0 |                        0 |                         0 |                      0 |                0 |                    0 |                 0 |                0 |               0 |                0 |\n",
    "|  1 |               0 |                 0 |                0 |                1 |               1 |                    1 |                               1 |                                       1 |            0 |              0 |            0 |              0 |               0 |                0 |                0 |                   0 |                  0 |                0 |             0 |                  0 |                      0 |                          0 |                     0 |                     0 |                   0 |                  0 |                  0 |                     0 |                       0 |                               0 |                            0 |                              0 |                        0 |                         0 |                      0 |                0 |                    0 |                 0 |                0 |               0 |                0 |\n",
    "|  2 |               0 |                 0 |                0 |                0 |               0 |                    0 |                               0 |                                       0 |            1 |              1 |            1 |              1 |               1 |                1 |                1 |                   1 |                  1 |                1 |             1 |                  1 |                      0 |                          0 |                     0 |                     0 |                   0 |                  0 |                  0 |                     0 |                       0 |                               0 |                            0 |                              0 |                        0 |                         0 |                      0 |                0 |                    0 |                 0 |                0 |               0 |                0 |\n",
    "|  3 |               0 |                 0 |                0 |                0 |               0 |                    0 |                               0 |                                       0 |            0 |              0 |            0 |              0 |               0 |                0 |                0 |                   0 |                  0 |                0 |             0 |                  0 |                      1 |                          1 |                     1 |                     1 |                   1 |                  1 |                  1 |                     1 |                       1 |                               1 |                            1 |                              1 |                        1 |                         1 |                      1 |                0 |                    0 |                 0 |                0 |               0 |                0 |\n",
    "|  4 |               0 |                 0 |                0 |                0 |               0 |                    0 |                               0 |                                       0 |            0 |              0 |            0 |              0 |               0 |                0 |                0 |                   0 |                  0 |                0 |             0 |                  0 |                      0 |                          0 |                     0 |                     0 |                   0 |                  0 |                  0 |                     0 |                       0 |                               0 |                            0 |                              0 |                        0 |                         0 |                      0 |                1 |                    1 |                 1 |                1 |               1 |                1 |\n",
    "\n",
    "````\n",
    "\n",
    "````{tabbed} Skip-1-gram\n",
    "```python3\n",
    "skip_1_gram_vocab = {}\n",
    "for sent in review_word_stem_tokens:\n",
    "    for word_idx in range(len(sent) - 2):\n",
    "        skipgram = sent[word_idx] + str(\" \") + sent[word_idx + 2]\n",
    "        if skipgram not in skip_1_gram_vocab:\n",
    "            skip_1_gram_vocab[skipgram] = len(skip_1_gram_vocab)\n",
    "\n",
    "review_sent_skipgram_count_vectors = np.zeros((len(review_word_stem_tokens), len(skip_1_gram_vocab)), dtype = np.int32)\n",
    "\n",
    "for sent_idx in range(len(review_word_stem_tokens)):\n",
    "    for word_idx in range(len(review_word_stem_tokens[sent_idx]) - 2):\n",
    "        skipgram = review_word_stem_tokens[sent_idx][word_idx] + str(\" \") + review_word_stem_tokens[sent_idx][word_idx + 2]\n",
    "        review_sent_skipgram_count_vectors[sent_idx][skip_1_gram_vocab[skipgram]] += 1\n",
    "\n",
    "df = pd.DataFrame(data = review_sent_skipgram_count_vectors, columns = sorted(skip_1_gram_vocab, key=skip_1_gram_vocab.get))\n",
    "display(df)\n",
    "```\n",
    "\n",
    "```python3\n",
    "print(df.to_markdown())\n",
    "```\n",
    "\n",
    "|    |   I sci-fi |   love put |   sci-fi lot |   sci-fi TV |   movi usual |   TV underfund |   usual under-appreci |   underfund misunderstood |   I like |   tri thi |   like I |   thi realli |   I good |   realli TV |   good sci-fi |   TV babylon |   sci-fi 5 |   babylon star |   5 trek |   star origin |   silli cheap |   prosthet cardboard |   cheap set |   cardboard stilt |   set dialogu |   stilt CG |   dialogu match |   CG background |   match pain |   background one-dimension |   pain charact |   one-dimension cannot |   charact overcom |   cannot 'sci-fi' |   overcom set |   i'm think |   sure babylon |   think 5 |   babylon good |   5 sci-fi |   good TV |\n",
    "|---:|-----------:|-----------:|-------------:|------------:|-------------:|---------------:|----------------------:|--------------------------:|---------:|----------:|---------:|-------------:|---------:|------------:|--------------:|-------------:|-----------:|---------------:|---------:|--------------:|--------------:|---------------------:|------------:|------------------:|--------------:|-----------:|----------------:|----------------:|-------------:|---------------------------:|---------------:|-----------------------:|------------------:|------------------:|--------------:|------------:|---------------:|----------:|---------------:|-----------:|----------:|\n",
    "|  0 |          1 |          1 |            1 |           0 |            0 |              0 |                     0 |                         0 |        0 |         0 |        0 |            0 |        0 |           0 |             0 |            0 |          0 |              0 |        0 |             0 |             0 |                    0 |           0 |                 0 |             0 |          0 |               0 |               0 |            0 |                          0 |              0 |                      0 |                 0 |                 0 |             0 |           0 |              0 |         0 |              0 |          0 |         0 |\n",
    "|  1 |          0 |          0 |            0 |           1 |            1 |              1 |                     1 |                         1 |        0 |         0 |        0 |            0 |        0 |           0 |             0 |            0 |          0 |              0 |        0 |             0 |             0 |                    0 |           0 |                 0 |             0 |          0 |               0 |               0 |            0 |                          0 |              0 |                      0 |                 0 |                 0 |             0 |           0 |              0 |         0 |              0 |          0 |         0 |\n",
    "|  2 |          0 |          0 |            0 |           0 |            0 |              0 |                     0 |                         0 |        1 |         1 |        1 |            1 |        1 |           1 |             1 |            1 |          1 |              1 |        1 |             1 |             0 |                    0 |           0 |                 0 |             0 |          0 |               0 |               0 |            0 |                          0 |              0 |                      0 |                 0 |                 0 |             0 |           0 |              0 |         0 |              0 |          0 |         0 |\n",
    "|  3 |          0 |          0 |            0 |           0 |            0 |              0 |                     0 |                         0 |        0 |         0 |        0 |            0 |        0 |           0 |             0 |            0 |          0 |              0 |        0 |             0 |             1 |                    1 |           1 |                 1 |             1 |          1 |               1 |               1 |            1 |                          1 |              1 |                      1 |                 1 |                 1 |             1 |           0 |              0 |         0 |              0 |          0 |         0 |\n",
    "|  4 |          0 |          0 |            0 |           0 |            0 |              0 |                     0 |                         0 |        0 |         0 |        0 |            0 |        0 |           0 |             0 |            0 |          0 |              0 |        0 |             0 |             0 |                    0 |           0 |                 0 |             0 |          0 |               0 |               0 |            0 |                          0 |              0 |                      0 |                 0 |                 0 |             0 |           1 |              1 |         1 |              1 |          1 |         1 |\n",
    "````\n",
    "\n",
    "```{tip}\n",
    "In case of a limited vocabulary size of $V$, we can use take the most-frequent $V - 1$ grams in the vocabulary and use a special gram/symbol `?` for the remaning grams.\n",
    "```\n",
    "#### Overall\n",
    "\n",
    "````{tabbed} Initially\n",
    "```\n",
    "I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn't match the background, and painfully one-dimensional characters cannot be overcome with a 'sci-fi' setting. (I'm sure there are those of you out there who think Babylon 5 is good sci-fi TV. It's not. It's clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It's really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it's rubbish as they have to always say \"Gene Roddenberry's Earth...\" otherwise people would not continue watching. Roddenberry's ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.\n",
    "```\n",
    "````\n",
    "\n",
    "````{tabbed} Into sentences\n",
    "```\n",
    "['I love sci-fi and am willing to put up with a lot.',\n",
    " 'Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood.',\n",
    " 'I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original).',\n",
    " \"Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn't match the background, and painfully one-dimensional characters cannot be overcome with a 'sci-fi' setting.\",\n",
    " \"(I'm sure there are those of you out there who think Babylon 5 is good sci-fi TV.\",\n",
    " \"It's not.\",\n",
    " \"It's clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf.\",\n",
    " 'Star Trek).',\n",
    " 'It may treat important issues, yet not as a serious philosophy.',\n",
    " \"It's really difficult to care about the characters here as they are not simply foolish, just missing a spark of life.\",\n",
    " 'Their actions and reactions are wooden and predictable, often painful to watch.',\n",
    " 'The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching.',\n",
    " \"Roddenberry's ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space.\",\n",
    " 'Spoiler.',\n",
    " 'So, kill off a main character.',\n",
    " 'And then bring him back as another actor.',\n",
    " 'Jeeez! Dallas all over again.']\n",
    "```\n",
    "````\n",
    "\n",
    "````{tabbed} Tokenization\n",
    "```\n",
    "['I', 'love', 'sci-fi', 'and', 'am', 'willing', 'to', 'put', 'up', 'with', 'a', 'lot']\n",
    "['Sci-fi', 'movies', 'TV', 'are', 'usually', 'underfunded', 'under-appreciated', 'and', 'misunderstood']\n",
    "['I', 'tried', 'to', 'like', 'this', 'I', 'really', 'did', 'but', 'it', 'is', 'to', 'good', 'TV', 'sci-fi', 'as', 'Babylon', '5', 'is', 'to', 'Star', 'Trek', 'the', 'original']\n",
    "['Silly', 'prosthetics', 'cheap', 'cardboard', 'sets', 'stilted', 'dialogues', 'CG', 'that', \"doesn't\", 'match', 'the', 'background', 'and', 'painfully', 'one-dimensional', 'characters', 'cannot', 'be', 'overcome', 'with', 'a', \"'sci-fi'\", 'setting']\n",
    "[\"I'm\", 'sure', 'there', 'are', 'those', 'of', 'you', 'out', 'there', 'who', 'think', 'Babylon', '5', 'is', 'good', 'sci-fi', 'TV']\n",
    "```\n",
    "````\n",
    "\n",
    "\n",
    "````{tabbed} Stemming\n",
    "```\n",
    "['I', 'love', 'sci-fi', 'and', 'am', 'will', 'to', 'put', 'up', 'with', 'a', 'lot']\n",
    "['sci-fi', 'movi', 'TV', 'are', 'usual', 'underfund', 'under-appreci', 'and', 'misunderstood']\n",
    "['I', 'tri', 'to', 'like', 'thi', 'I', 'realli', 'did', 'but', 'it', 'is', 'to', 'good', 'TV', 'sci-fi', 'as', 'babylon', '5', 'is', 'to', 'star', 'trek', 'the', 'origin']\n",
    "['silli', 'prosthet', 'cheap', 'cardboard', 'set', 'stilt', 'dialogu', 'CG', 'that', \"doesn't\", 'match', 'the', 'background', 'and', 'pain', 'one-dimension', 'charact', 'cannot', 'be', 'overcom', 'with', 'a', \"'sci-fi'\", 'set']\n",
    "[\"i'm\", 'sure', 'there', 'are', 'those', 'of', 'you', 'out', 'there', 'who', 'think', 'babylon', '5', 'is', 'good', 'sci-fi', 'TV']\n",
    "```\n",
    "````\n",
    "\n",
    "\n",
    "````{tabbed} Remove Stop words\n",
    "```\n",
    "['I', 'love', 'sci-fi', 'put', 'lot']\n",
    "['sci-fi', 'movi', 'TV', 'usual', 'underfund', 'under-appreci', 'misunderstood']\n",
    "['I', 'tri', 'like', 'thi', 'I', 'realli', 'good', 'TV', 'sci-fi', 'babylon', '5', 'star', 'trek', 'origin']\n",
    "['silli', 'prosthet', 'cheap', 'cardboard', 'set', 'stilt', 'dialogu', 'CG', 'match', 'background', 'pain', 'one-dimension', 'charact', 'cannot', 'overcom', \"'sci-fi'\", 'set']\n",
    "[\"i'm\", 'sure', 'think', 'babylon', '5', 'good', 'sci-fi', 'TV']\n",
    "```\n",
    "````\n",
    "\n",
    "\n",
    "````{tabbed} Bigrams\n",
    "|    |   I love |   love sci-fi |   sci-fi put |   put lot |   sci-fi movi |   movi TV |   TV usual |   usual underfund |   underfund under-appreci |   under-appreci misunderstood |   I tri |   tri like |   like thi |   thi I |   I realli |   realli good |   good TV |   TV sci-fi |   sci-fi babylon |   babylon 5 |   5 star |   star trek |   trek origin |   silli prosthet |   prosthet cheap |   cheap cardboard |   cardboard set |   set stilt |   stilt dialogu |   dialogu CG |   CG match |   match background |   background pain |   pain one-dimension |   one-dimension charact |   charact cannot |   cannot overcom |   overcom 'sci-fi' |   'sci-fi' set |   i'm sure |   sure think |   think babylon |   5 good |   good sci-fi |   sci-fi TV |\n",
    "|---:|---------:|--------------:|-------------:|----------:|--------------:|----------:|-----------:|------------------:|--------------------------:|------------------------------:|--------:|-----------:|-----------:|--------:|-----------:|--------------:|----------:|------------:|-----------------:|------------:|---------:|------------:|--------------:|-----------------:|-----------------:|------------------:|----------------:|------------:|----------------:|-------------:|-----------:|-------------------:|------------------:|---------------------:|------------------------:|-----------------:|-----------------:|-------------------:|---------------:|-----------:|-------------:|----------------:|---------:|--------------:|------------:|\n",
    "|  0 |        1 |             1 |            1 |         1 |             0 |         0 |          0 |                 0 |                         0 |                             0 |       0 |          0 |          0 |       0 |          0 |             0 |         0 |           0 |                0 |           0 |        0 |           0 |             0 |                0 |                0 |                 0 |               0 |           0 |               0 |            0 |          0 |                  0 |                 0 |                    0 |                       0 |                0 |                0 |                  0 |              0 |          0 |            0 |               0 |        0 |             0 |           0 |\n",
    "|  1 |        0 |             0 |            0 |         0 |             1 |         1 |          1 |                 1 |                         1 |                             1 |       0 |          0 |          0 |       0 |          0 |             0 |         0 |           0 |                0 |           0 |        0 |           0 |             0 |                0 |                0 |                 0 |               0 |           0 |               0 |            0 |          0 |                  0 |                 0 |                    0 |                       0 |                0 |                0 |                  0 |              0 |          0 |            0 |               0 |        0 |             0 |           0 |\n",
    "|  2 |        0 |             0 |            0 |         0 |             0 |         0 |          0 |                 0 |                         0 |                             0 |       1 |          1 |          1 |       1 |          1 |             1 |         1 |           1 |                1 |           1 |        1 |           1 |             1 |                0 |                0 |                 0 |               0 |           0 |               0 |            0 |          0 |                  0 |                 0 |                    0 |                       0 |                0 |                0 |                  0 |              0 |          0 |            0 |               0 |        0 |             0 |           0 |\n",
    "|  3 |        0 |             0 |            0 |         0 |             0 |         0 |          0 |                 0 |                         0 |                             0 |       0 |          0 |          0 |       0 |          0 |             0 |         0 |           0 |                0 |           0 |        0 |           0 |             0 |                1 |                1 |                 1 |               1 |           1 |               1 |            1 |          1 |                  1 |                 1 |                    1 |                       1 |                1 |                1 |                  1 |              1 |          0 |            0 |               0 |        0 |             0 |           0 |\n",
    "|  4 |        0 |             0 |            0 |         0 |             0 |         0 |          0 |                 0 |                         0 |                             0 |       0 |          0 |          0 |       0 |          0 |             0 |         0 |           0 |                0 |           1 |        0 |           0 |             0 |                0 |                0 |                 0 |               0 |           0 |               0 |            0 |          0 |                  0 |                 0 |                    0 |                       0 |                0 |                0 |                  0 |              0 |          1 |            1 |               1 |        1 |             1 |           1 |\n",
    "````\n",
    "\n",
    "<footer>\n",
    "Author(s): Sachin Yadav\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   11,
   125,
   131,
   136
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}